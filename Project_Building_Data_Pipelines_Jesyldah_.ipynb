{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project : Building Data Pipelines - Jesyldah_",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jesyldah/Projects/blob/main/Project_Building_Data_Pipelines_Jesyldah_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFKyiuxG9yPw"
      },
      "source": [
        "# Project Notebook: Building Data Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzRQzJDP92yt"
      },
      "source": [
        "## 1. Introduction to the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gT_nUX69632"
      },
      "source": [
        "In this past, we began with the concepts of functional programming, and then built our own data pipeline class in Python. We learned about advanced Python concepts such as the decorators and closures. In the last session, we also learned how to implement a directed acyclic graph as the scheduler for our pipeline.\n",
        "\n",
        "After completing all these sessions, we have finally built a robust data pipeline that schedules our tasks in the correct order! In this project, we will use the pipeline we have been building, and apply it to a real world data pipeline project.\n",
        "\n",
        "From a JSON API, we will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for us.\n",
        "\n",
        "The data we will use comes from a Hacker News (HN) API that returns JSON data of the top stories in 2014. If you're unfamiliar with Hacker News, it's a link aggregator website that users vote up stories that are interesting to the community. It is similar to Reddit, but the community only revolves around on computer science and entrepreneurship posts.\n",
        "\n",
        "To make things easier, we have already downloaded a list of JSON posts to a file called `hn_stories_2014.json` (https://bit.ly/32EDt65). The JSON file contains a single key stories, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
        "\n",
        "* `created_at`: A timestamp of the story's creation time.\n",
        "* `created_at_i`: A unix epoch timestamp.\n",
        "* `url`: The URL of the story link.\n",
        "* `objectID`: The ID of the story.\n",
        "* `author`: The story's author (username on HN).\n",
        "* `points`: The number of upvotes the story had.\n",
        "* `title`: The headline of the post.\n",
        "* `num_comments`: The number of a comments a post has.\n",
        "\n",
        "Here's an example of the full list of keys in a story:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"story_text\": \"\",\n",
        "    \"created_at\": \"2014-05-29T08:23:46Z\",\n",
        "    \"story_title\": null,\n",
        "    \"story_id\": null,\n",
        "    \"comment_text\": null,\n",
        "    \"created_at_i\": 1401351826,\n",
        "    \"url\": \"http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/\",\n",
        "    \"parent_id\": null,\n",
        "    \"objectID\": \"7815285\",\n",
        "    \"author\": \"Leynos\",\n",
        "    \"points\": 1,\n",
        "    \"title\": \"Making Twitter Easier to Use\",\n",
        "    \"_tags\": [\n",
        "        \"story\",\n",
        "        \"author_Leynos\",\n",
        "        \"story_7815285\"\n",
        "    ],\n",
        "    \"num_comments\": 0,\n",
        "    \"_highlightResult\": {\n",
        "        \"story_text\": {\n",
        "            \"matchedWords\": [],\n",
        "            \"value\": \"\",\n",
        "            \"matchLevel\": \"none\"\n",
        "        },\n",
        "        \"author\": {\n",
        "            \"matchedWords\": [],\n",
        "            \"value\": \"Leynos\",\n",
        "            \"matchLevel\": \"none\"\n",
        "        },\n",
        "        \"url\": {\n",
        "            \"matchedWords\": [],\n",
        "            \"value\": \"http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/\",\n",
        "            \"matchLevel\": \"none\"\n",
        "        },\n",
        "        \"title\": {\n",
        "            \"matchedWords\": [],\n",
        "            \"value\": \"Making Twitter Easier to Use\",\n",
        "            \"matchLevel\": \"none\"\n",
        "        }\n",
        "    },\n",
        "    \"story_url\": null\n",
        "}\n",
        "```\n",
        "\n",
        "Using this dataset, we will run a sequence of basic natural language processing tasks using our Pipeline class. The goal will be to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014!\n",
        "\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "* Import the Pipeline class from the pipeline module. You can import it like so: `from pipeline import Pipeline`. But before doing this, download the zip file (https://bit.ly/310BfgJ) that contains a file that contains this Class. Then upload it to google colab, like you do with other dataset files.\n",
        "\n",
        "* Instantiate an instance of the `Pipeline` class and assign it to the variable `pipeline`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnMn4fYXWAs9"
      },
      "source": [
        "### Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI_Kzrlz9bzO",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "ca70a0f7-d66e-4b5e-838f-80b7fc5e9c9d"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d3272898-55a8-434f-b911-d6d1c16382a9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d3272898-55a8-434f-b911-d6d1c16382a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pipeline.py to pipeline.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pipeline.py': b\"import csv\\nfrom collections import deque\\nimport itertools\\n\\n\\nclass DAG:\\n    def __init__(self):\\n        self.graph = {}\\n\\n    def in_degrees(self):\\n        in_degrees = {}\\n        for node in self.graph:\\n            if node not in in_degrees:\\n                in_degrees[node] = 0\\n            for pointed in self.graph[node]:\\n                if pointed not in in_degrees:\\n                    in_degrees[pointed] = 0\\n                in_degrees[pointed] += 1\\n        return in_degrees\\n\\n    def sort(self):\\n        in_degrees = self.in_degrees()\\n        to_visit = deque()\\n        for node in self.graph:\\n            if in_degrees[node] == 0:\\n                to_visit.append(node)\\n\\n        searched = []\\n        while to_visit:\\n            node = to_visit.popleft()\\n            for pointer in self.graph[node]:\\n                in_degrees[pointer] -= 1\\n                if in_degrees[pointer] == 0:\\n                    to_visit.append(pointer)\\n            searched.append(node)\\n        return searched\\n\\n    def add(self, node, to=None):\\n        if node not in self.graph:\\n            self.graph[node] = []\\n        if to:\\n            if to not in self.graph:\\n                self.graph[to] = []\\n            self.graph[node].append(to)\\n        if len(self.sort()) != len(self.graph):\\n            raise Exception\\n\\n\\nclass Pipeline:\\n    def __init__(self):\\n        self.tasks = DAG()\\n\\n    def task(self, depends_on=None):\\n        def inner(f):\\n            self.tasks.add(f)\\n            if depends_on:\\n                self.tasks.add(depends_on, f)\\n            return f\\n        return inner\\n\\n    def run(self):\\n        scheduled = self.tasks.sort()\\n        completed = {}\\n\\n        for task in scheduled:\\n            for node, values in self.tasks.graph.items():\\n                if task in values:\\n                    completed[task] = task(completed[node])\\n            if task not in completed:\\n                completed[task] = task()\\n        return completed\\n\\n\\ndef build_csv(lines, header=None, file=None):\\n    if header:\\n        lines = itertools.chain([header], lines)\\n    writer = csv.writer(file, delimiter=',')\\n    writer.writerows(lines)\\n    file.seek(0)\\n    return file\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZinS9LTAVxqN"
      },
      "source": [
        "from pipeline import Pipeline"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0BajGWFV7mh"
      },
      "source": [
        "pipeline = Pipeline()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mQb_7Xy-2ZX"
      },
      "source": [
        "## 2. Loading the JSON Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bb33xp-9qg"
      },
      "source": [
        "We'll start the project by loading the JSON file data into Python. Because JSON files resemble a key-value dictionary, the goal is to parse the JSON file into a Python `dict` object. We can accomplish this using the `json` module.\n",
        "\n",
        "In a previous session, we worked with this JSON parser before. As a reminder, this is how you can parse JSON strings:\n",
        "\n",
        "```\n",
        "import json\n",
        "​\n",
        "# Notice that `sample_json` is a string, and\n",
        "# NOT a dict.\n",
        "sample_json = '{\"hello\": \"world\"}'\n",
        "sample_dict = json.loads(sample_json)\n",
        "print(sample_dict)\n",
        "{'hello': 'world'}\n",
        "```\n",
        "\n",
        "To load in a file, json exposes a method called `json.load()` which takes in a Python file object as the first argument. Using this `json.load()` method, we'll load the `hn_stories_2014.json` file as a Python dict.\n",
        "\n",
        "\n",
        "**Tasks**\n",
        "1. Create a `pipeline.task()` function that takes in no arguments.\n",
        "2. Call the function `file_to_json()`, where the function does the following:\n",
        "* Loads the `hn_stories_2014.json` file into a Python dict.\n",
        "* Returns the list of `stories`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBDHfgRQ8Ztq"
      },
      "source": [
        "### Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "4OHqhKoLWrVT",
        "outputId": "61ce1dfe-70c2-4288-987e-1792fe02376a"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17468c45-4eb7-458a-aabd-a1e5b66d0041\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-17468c45-4eb7-458a-aabd-a1e5b66d0041\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving hn_stories_2014.json to hn_stories_2014.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtoPo1jI_Z4J"
      },
      "source": [
        "import json\n",
        "\n",
        "@pipeline.task()\n",
        "def file_to_json():\n",
        "  \"\"\"Reads a json file into a python dictionary\n",
        "  \n",
        "  Returns:\n",
        "    List\n",
        "  \"\"\"\n",
        "  f = open('hn_stories_2014.json')\n",
        "  data = json.load(f)\n",
        "  stories = data['stories']\n",
        "  return stories\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6aze2Kk_dwZ"
      },
      "source": [
        "## 3. Filtering the Stories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZN5q8BD_het"
      },
      "source": [
        "Great! Now that we have loaded in all the stories as a list of `dict` objects, we can now operate on them. Let's start by filtering the list of stories to get the most popular stories of the year.\n",
        "\n",
        "Like any social link aggregator site, individual users can post whatever content they want. The reason we want the most popular stories is to ensure that we select stories that were the most talked about during the year. We can filter for popular stories by ensuring they are links (not `Ask HN` posts), have a good number of points, and have some comments.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "* Create a `pipeline.task()` function that depends on the `file_to_json()` function.\n",
        "* Call the new function `filter_stories()`, that filters popular stories that have more than 50 points, more than 1 comment, and do not begin with `Ask HN`.\n",
        "* `filter_stories()` should return a generator of these filtered stories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYPvbL15YSYr"
      },
      "source": [
        "### Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odn6TOet_gIj"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def parse_time(time_str):\n",
        "    \"\"\"\n",
        "    Parses time in the format [2014-05-29T08:23:46Z]\n",
        "    to a datetime object.\n",
        "    \"\"\"\n",
        "    time_obj = datetime(int(time_str[0:4]),int(time_str[5:7]),int(time_str[8:10]))\n",
        "    return time_obj\n",
        "\n",
        "@pipeline.task(depends_on=file_to_json)\n",
        "def filter_stories(stories):\n",
        "  \"\"\"Filters lines from a file\n",
        "\n",
        "  Args:\n",
        "    stories(str): The string to filter\n",
        "\n",
        "  Returns:\n",
        "    str\n",
        "  \"\"\"\n",
        "  for story in stories:\n",
        "    if (story['points'] > 50) & (story['num_comments'] > 1) & (not story['title'].startswith('Ask HN')):\n",
        "      objectID = story['objectID']\n",
        "      created_at = parse_time(story['created_at'])\n",
        "      url = story['url']\n",
        "      points = story['points']\n",
        "      title = story['title']\n",
        "      yield (objectID,created_at,url,points,title)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKlzGqGc_9Go"
      },
      "source": [
        "## 4. Convert to CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBYnNaA8AAbj"
      },
      "source": [
        "With a reduced set of stories, it's time to write these `dict` objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistent data format when running the later summarizations. By keeping consistent data formats, each of your pipeline tasks will be adaptable with future task requirements.\n",
        "\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Create a `pipeline.task()` function that depends on the `filter_stories()` function.\n",
        "2. Call the new function `json_to_csv()`, that writes the filtered JSON stories to a CSV file:\n",
        "* Import `build_csv` from the `pipeline` module and `io`. The `build_csv()` function has the same API as the one you wrote in the second and third lesson.\n",
        "* Create a CSV file with the headers `'objectID', 'created_at', 'url', 'points', and 'title'`.\n",
        "* Parse the `created_at` column using `datetime.datetime`.\n",
        "3. `json_to_csv()` should return the value from `build_csv()` using the above header, lines, and the `io.StringIO()` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR4jl7HvYVDH"
      },
      "source": [
        "### Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2yUUdI1AafQ"
      },
      "source": [
        "from pipeline import build_csv\n",
        "\n",
        "import io\n",
        "\n",
        "@pipeline.task(depends_on=filter_stories)\n",
        "def json_to_csv(filtered_stories):\n",
        "  \"\"\"Creates a csv file from a lines\n",
        "\n",
        "  Args:\n",
        "    filtered_stories(str): Lines to write to the csv file\n",
        "\n",
        "  Returns:\n",
        "    str: _io.stringIO instance\n",
        "  \"\"\"\n",
        "  return build_csv(filtered_stories, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcNzGr7AAhpV"
      },
      "source": [
        "## 5. Extract Title Column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrIsv8JlAi_x"
      },
      "source": [
        "Using the CSV file format we created in the previous task, we can now extract the title column. Once we have extracted the titles of each popular post, we can then run the next word frequency task. To extract the titles, we'll follow the steps in the tasks we wrote in the past.\n",
        "\n",
        "The steps were: **1.** Import csv, and create a `csv.reader()` object from the file object. **2.** Find the index of the title in the header. **3.** Iterate the through the reader, and return each item from the reader in the corresponding title index position.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Create a `pipeline.task()` function that depends on the `json_to_csv()` function.\n",
        "2. Call the new function `extract_titles()`, that returns of a generator of every Hacker News story title:\n",
        "* Follow the steps listed in the instructions.\n",
        "3. `extract_titles()` should return a generator of titles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvBb7-8EYXeD"
      },
      "source": [
        "### Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsiFTvdRAu8P"
      },
      "source": [
        "import csv\n",
        "\n",
        "import itertools\n",
        "\n",
        "@pipeline.task(depends_on=json_to_csv)\n",
        "def extract_titles(csv_file):\n",
        "    \"\"\"Generates a string from a file\n",
        "\n",
        "    Args:\n",
        "      csv_file(str): File to read from\n",
        "\n",
        "    Returns:\n",
        "      str: String from csv file\n",
        "    \"\"\"\n",
        "    reader = csv.reader(csv_file)\n",
        "    header = next(reader)\n",
        "    idx = header.index('title')\n",
        "    for line in reader:\n",
        "      yield line[idx]\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dnd0tf0BCfq"
      },
      "source": [
        "## 6. Clean the Titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2toQ-M8NBWCU"
      },
      "source": [
        "Because we're trying to create a word frequency model of words from Hacker News titles, we need a way to create a consistent set of words to use. For example, words like `Google, google, GooGle?, and google.`, all mean the same keyword: google. If we were to split the title into words, however, they would all be lumped into different categories.\n",
        "\n",
        "To clean the titles, we should make sure to lower case the titles, and to remove the punctuation. An easy way to rid a string of punctuation is to check each character, determine if it is a letter or punctuation, and only keep the letter. From the string package, we are given a handy string constant that contains all the punctuation needed:\n",
        "\n",
        "```\n",
        "import string\n",
        "​\n",
        "print(string.punctuation)\n",
        "'!\"#%&'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "```\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Create a `pipeline.task()` function that depends on the `extract_titles()` function.\n",
        "2. Call the new function `clean_titles()`, that returns of a generator of cleaned titles:\n",
        "* Ensure the title is lower case.\n",
        "* Remove any punctuation from the title."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfqmhU4Vg2CV"
      },
      "source": [
        "### Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ7PUnPQBLGv"
      },
      "source": [
        "import string\n",
        "\n",
        "@pipeline.task(depends_on=extract_titles)\n",
        "def clean_titles(titles):\n",
        "  \"\"\"Generates a string from a file with all punctuations removed\n",
        "\n",
        "    Args:\n",
        "      titles (str)\n",
        "\n",
        "    Returns:\n",
        "      str\n",
        "  \"\"\"\n",
        "  for title in titles:\n",
        "    clean_string = ''.join([i for i in title.lower() if i not in string.punctuation])\n",
        "    yield clean_string\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-wvOCtLBv7A"
      },
      "source": [
        "## 7. Create the Word Frequency Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N-VYShXByPK"
      },
      "source": [
        "With a cleaned title, we can now build the word frequency dictionary. A word frequency dictionary are key value pairs that connects a word to the number of times it is used in a text. Here's an example of how a word frequency would work on a single string:\n",
        "\n",
        "```\n",
        "sample_text = \"Wow, the Dataquest Data Engineering track is the best track!\"\n",
        "print(word_freq_from_string(sample_text))\n",
        "\n",
        "{'wow': 1, 'the': 2, 'dataquest': 1, 'data': 1, 'engineering': 1, 'track': 2, 'is': 1, 'best': 1}\n",
        "```\n",
        "\n",
        "As you can see, the title has been stripped of its punctuation and lower cased. Furthermore, to find actual keywords, we should enforce the word frequency dictionary to not include stop words. Stop words are words that occur frequently in language like `\"the\", \"or\", etc.`, and are commonly rejected in keyword searches.\n",
        "\n",
        "We have included a module called `stop_words` (https://bit.ly/3nOTeiJ) with a tuple of the most common used stop words in the English language. You can import in your notebook by using from `stop_words` import `stop_words`. Download it, extract it, and upload it to colab.\n",
        "\n",
        "You can import in your notebook by using from `stop_words` import `stop_words`. Here's what the sample text would look like without the stop words:\n",
        "\n",
        "```\n",
        "sample_text = \"Wow, the Dataquest Data Engineering track is the best track!\"\n",
        "print(word_freq_no_stop_words(sample_text))\n",
        "\n",
        "{'wow': 1, 'dataquest': 1, 'data': 1, 'engineering': 1, 'track': 2, 'best': 1}\n",
        "```\n",
        "\n",
        "**Tasks**\n",
        "1. Create a `pipeline.task()` function that depends on the `clean_titles()` function.\n",
        "2. Call the new function `build_keyword_dictionary()`, that returns a dictionary of the word frequency of all the HN titles.\n",
        "* The word frequency should **not** include stop words.\n",
        "* You can find the words by spliting the titles dictionary on the empty space character .\n",
        "* Empty words **should** be ignored.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1CZlGozioQW"
      },
      "source": [
        "### Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QBBHmU4Ccjz",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "17d71f44-02eb-4773-ba59-805d7ebab2ab"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-61d5a7ba-f6f5-42c0-8736-6f6ad3cee38e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-61d5a7ba-f6f5-42c0-8736-6f6ad3cee38e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCCOOjNMlVyh",
        "outputId": "d9b9f438-f1e1-4933-c63a-c43c82b62822"
      },
      "source": [
        "from stop_words import stop_words\n",
        "\n",
        "print(stop_words)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', 'about', 'above', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'the', '–', 'hn')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipcpD_p9iw3M"
      },
      "source": [
        "@pipeline.task(depends_on=clean_titles)\n",
        "def build_keyword_dictionary(clean_titles):\n",
        "  \"\"\"Counts the number of times a word appears in a list of strings\n",
        "\n",
        "    Args:\n",
        "      clean_titles(str)\n",
        "\n",
        "    Returns:\n",
        "      dict{str, int}\n",
        "  \"\"\"\n",
        "  word_frequency = {}\n",
        "  for line in clean_titles:\n",
        "    words = [w for w in line.split() if not w in stop_words]\n",
        "    for word in words:\n",
        "      if not word_frequency.get(word):\n",
        "        word_frequency[word] = 0\n",
        "      word_frequency[word] += 1\n",
        "  return word_frequency\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB_cULLWCisd"
      },
      "source": [
        "## 8. Sort the Top Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPb8mQlBClvi"
      },
      "source": [
        "Finally, we're ready to sort the top words used in all the titles. In this final task, it's up to you to decide how you want to sort the top words. The goal is to output a list of tuples with (`word`, `frequency`) as the entries sorted from most used, to least most used.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Create a `pipeline.task()` function that depends on the `build_keyword_dictionary()` function.\n",
        "2. The new function can be named whatever you want, but it should return a list of the top 100 tuples described in the explanation above.\n",
        "3. Run the pipline using `pipeline.run()`, and print the ouput of the new task function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg3QZ2-t8QoR"
      },
      "source": [
        "### Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-WSkritC0BQ"
      },
      "source": [
        "@ pipeline.task(depends_on=build_keyword_dictionary)\n",
        "def top_words_sort(word_frequency):\n",
        "  \"\"\"Sorts a dictionary\n",
        "\n",
        "    Args:\n",
        "      word_frequency(dict)\n",
        "\n",
        "    Returns:\n",
        "      list \n",
        "  \"\"\"\n",
        "  sort_tuple = ()\n",
        "  sort_list = []\n",
        "  for key,value in word_frequency.items():\n",
        "      sort_tuple = (key, value)\n",
        "      sort_list.append(sort_tuple)\n",
        "  return (sorted(sort_list, key = lambda x: x[1],reverse=True)[0:100])\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2jnmUOItrak"
      },
      "source": [
        "Combining the tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnVcDMqjOq5R"
      },
      "source": [
        "from pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline()\n",
        "\n",
        "import json\n",
        "\n",
        "@pipeline.task()\n",
        "def file_to_json():\n",
        "  f = open('hn_stories_2014.json')\n",
        "  data = json.load(f)\n",
        "  stories = data['stories']\n",
        "  return stories\n",
        "  \n",
        "from datetime import datetime\n",
        "\n",
        "def parse_time(time_str):\n",
        "    \"\"\"\n",
        "    Parses time in the format [30/Nov/2017:11:59:54 +0000]\n",
        "    to a datetime object.\n",
        "    \"\"\"\n",
        "    time_obj = datetime(int(time_str[0:4]),int(time_str[5:7]),int(time_str[8:10]))\n",
        "    return time_obj\n",
        "\n",
        "@pipeline.task(depends_on=file_to_json)\n",
        "def filter_stories(stories):\n",
        "  for story in stories:\n",
        "    if (story['points'] > 50) & (story['num_comments'] > 1) & (not story['title'].startswith('Ask HN')):\n",
        "      objectID = story['objectID']\n",
        "      created_at = parse_time(story['created_at'])\n",
        "      url = story['url']\n",
        "      points = story['points']\n",
        "      title = story['title']\n",
        "      yield (objectID,created_at,url,points,title)\n",
        "\t  \n",
        "from pipeline import build_csv\n",
        "\n",
        "import io\n",
        "\n",
        "@pipeline.task(depends_on=filter_stories)\n",
        "def json_to_csv(filtered_stories):\n",
        "  return build_csv(filtered_stories, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n",
        "\n",
        "import csv\n",
        "\n",
        "import itertools\n",
        "\n",
        "@pipeline.task(depends_on=json_to_csv)\n",
        "def extract_titles(csv_file):\n",
        "    reader = csv.reader(csv_file)\n",
        "    header = next(reader)\n",
        "    idx = header.index('title')\n",
        "    for line in reader:\n",
        "      yield line[idx]\n",
        "\t  \n",
        "import string\n",
        "\n",
        "@pipeline.task(depends_on=extract_titles)\n",
        "def clean_titles(titles):\n",
        "  for title in titles:\n",
        "    clean_string = ''.join([i for i in title.lower() if i not in string.punctuation])\n",
        "    yield clean_string\n",
        "\n",
        "from stop_words import stop_words\n",
        "\n",
        "@pipeline.task(depends_on=clean_titles)\n",
        "def build_keyword_dictionary(clean_titles):\n",
        "  word_frequency = {}\n",
        "  for line in clean_titles:\n",
        "    words = [w for w in line.split() if not w in stop_words]\n",
        "    for word in words:\n",
        "      if not word_frequency.get(word):\n",
        "        word_frequency[word] = 0\n",
        "      word_frequency[word] += 1\n",
        "  return word_frequency\n",
        "  \n",
        "@pipeline.task(depends_on=build_keyword_dictionary)\n",
        "def top_words_sort(word_frequency):\n",
        "  sort_tuple = ()\n",
        "  sort_list = []\n",
        "  for key,value in word_frequency.items():\n",
        "      sort_tuple = (key, value)\n",
        "      sort_list.append(sort_tuple)\n",
        "  return (sorted(sort_list, key = lambda x: x[1],reverse=True)[0:100])\n",
        "\n",
        "top_words_sorted_list = pipeline.run()\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_4MJak9s97Y",
        "outputId": "652ddbc9-8ade-4e2a-b2fc-a2f5f27bcab1"
      },
      "source": [
        "# Display results\n",
        "top_100_words = top_words_sorted_list[top_words_sort]\n",
        "print(top_100_words)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('new', 185), ('google', 167), ('bitcoin', 101), ('open', 92), ('programming', 90), ('web', 88), ('data', 85), ('video', 79), ('python', 76), ('code', 72), ('facebook', 71), ('released', 71), ('using', 70), ('2013', 65), ('javascript', 65), ('free', 64), ('source', 64), ('game', 63), ('internet', 62), ('microsoft', 59), ('c', 59), ('linux', 58), ('app', 57), ('pdf', 55), ('work', 54), ('language', 54), ('software', 52), ('2014', 52), ('startup', 51), ('apple', 50), ('use', 50), ('make', 50), ('time', 48), ('yc', 48), ('security', 48), ('nsa', 45), ('github', 45), ('windows', 44), ('1', 41), ('world', 41), ('way', 41), ('like', 41), ('project', 40), ('computer', 40), ('heartbleed', 40), ('git', 37), ('users', 37), ('dont', 37), ('design', 37), ('ios', 37), ('developer', 36), ('os', 36), ('twitter', 36), ('ceo', 36), ('vs', 36), ('life', 36), ('big', 35), ('day', 35), ('android', 34), ('online', 34), ('years', 33), ('simple', 33), ('court', 33), ('guide', 32), ('learning', 32), ('mt', 32), ('api', 32), ('says', 32), ('apps', 32), ('browser', 32), ('server', 31), ('firefox', 31), ('fast', 31), ('amazon', 31), ('gox', 31), ('problem', 31), ('mozilla', 31), ('engine', 31), ('site', 31), ('introducing', 30), ('year', 30), ('support', 29), ('stop', 29), ('built', 29), ('better', 29), ('million', 29), ('people', 29), ('text', 29), ('3', 28), ('does', 28), ('tech', 28), ('development', 28), ('billion', 27), ('developers', 27), ('just', 27), ('library', 27), ('did', 27), ('website', 27), ('money', 27), ('inside', 27)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3axa8FJDLfr"
      },
      "source": [
        "## Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXThiJgaDQkB"
      },
      "source": [
        "The final result yielded some interesting keywords. There were terms like `bitcoin` (the cryptocurrency), `heartbleed` (the 2014 hack), and many others. Even though this was a basic natural language processing task, it did provide some interesting insights into conversations from 2014. Nonetheless, now that you have created the pipeline, there are additional tasks you can perform with the data.\n",
        "\n",
        "Here are just a few:\n",
        "\n",
        "* Rewrite the Pipeline class' output to save a file of the output for each task. This will allow you to \"checkpoint\" tasks so they don't have to be run twice.\n",
        "* Use the `nltk` package for more advanced natural language processing tasks.\n",
        "* Convert to a CSV before filtering, so you can keep all the stories from 2014 in a raw file.\n",
        "* Fetch the data from Hacker News directly from a JSON API. Instead of reading from the file we gave, you can perform additional data processing using newer data."
      ]
    }
  ]
}